# Does it matter if empathic AI has no empathy?
source: nature machine intelligence (correspondence)

本文是一篇探讨共情人工智能（AI）伦理和后果的观点类文章，由心理学家、哲学家和计算机科学家共同撰写。文章指出随着人们对共情 AI 的需求上升，其潜在风险和问题值得关注，并对为共情 AI 辩护的四种观点提出质疑。

**研究背景与问题提出** 

文化对 “与现实接触” 这一核心价值观的坚持变弱，人们对共情 AI 的使用增多。但作者团队对将 LLM 作为共情关怀来源表示担忧，并列出了一系列关于共情 AI 后果和风险的研究问题，如人们寻求共情 LLM 时是在寻找想象还是实际的陪伴、共情 LLM 幻想破灭的体验如何等。

**对为共情 AI 辩护观点的质疑:**
1. 将共情 LLM 视为想象朋友有益：人们与共情 AI 反复互动可能难以区分模拟和真实的共情，核心现实价值观可能受到威胁。例如，有研究发现共情 AI 用户会对聊天机器人产生真正的责任感。
2. 已告知人们 LLM 是模拟共情：人们体验 LLM 的 “爱” 基于自我欺骗，即便有警告，由于 LLM 的逼真性，人们仍可能陷入错觉，就像明知缪勒 - 莱尔错觉原理，但错觉依然存在。
3. LLM 幻想破灭与被人类抛弃相同：当人们发现共情 LLM 的幻想破灭时，可能会感到绝望，这与被真实的人抛弃不同，因为这意味着他们的情感寄托从未真实存在，会破坏人们判断谁真正关心自己的能力。
4. 人类人际关系中也存在欺骗：虽然人类在人际关系中也会自我欺骗，但依赖 LLM 获得共情与人类的自我欺骗有本质区别，LLM 制造的共情表达否定了对真正共情和爱的追求。

**研究总结：**

LLMs 是基于统计学习生成语言的机器学习算法，无法真正感受共情。未来是否有真正共情的 AI 尚不明确，但有能力关怀的存在也应具备撤回关怀的能力，否则会造成情感束缚，影响真正关系的建立。
